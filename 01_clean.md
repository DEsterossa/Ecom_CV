# Clean Pipeline - Предсказание alpha канала

## Описание

Скрипт для обучения модели предсказания alpha канала (значения 0..255) с использованием архитектуры U2NETP. Метрика качества - MSE (Mean Squared Error).

**Данные:**
- Обучающая выборка: MAGICK (train)
- Тестовая выборка: Kaggle test (orig_1024)

**Файлы:**
- `01_clean.py` - основной скрипт (конвертирован из `01_clean.ipynb`)
- `01_clean.ipynb` - оригинальный ноутбук

## Структура скрипта

### 1. Imports

Импортируются необходимые библиотеки:
- `torch`, `torchvision` - для работы с нейронными сетями
- `PIL`, `numpy`, `pandas` - для обработки данных
- `scipy.ndimage` - для постпроцессинга (компонентный анализ)
- `cv2` - для аугментации (GaussianBlur)
- `tqdm` - для прогресс-баров
- `matplotlib` - для визуализации (опционально)

### 2. CONFIG

Основные параметры конфигурации:

**Семена и устройство:**
- `SEED = 42` - для воспроизводимости
- `device` - автоматический выбор GPU/CPU
- Включены оптимизации: `tf32`, `cudnn`, `float32_matmul_precision="high"`

**Пути к данным:**
- `TRAIN_CSV = Path("data/splits/train.csv")` - CSV файл с путями к обучающим изображениям
- `VAL_CSV = Path("data/splits/val.csv")` - CSV файл с путями к валидационным изображениям
- `TEST_ROOT = Path("data/test_dataset/orig_1024")` - директория с тестовыми изображениями
- `MODEL_DIR = Path("outputs/experiments/exp_001")` - директория для сохранения моделей

**Параметры обучения:**
- `TRAIN_SIZE = (512, 512)` - размер изображений при обучении
- `VAL_SIZE = (1024, 1024)` - размер изображений при валидации
- `TRAIN_BATCH = 4` - размер батча для обучения
- `VAL_BATCH = 1` - размер батча для валидации
- `NUM_EPOCHS = 12` - количество эпох
- `LR = 5e-5` - начальная скорость обучения
- `LIMIT_DATA = 1000` - ограничение количества данных (для отладки)
- `RUN_TRAINING = True` - флаг для запуска обучения (False - только инференс)

### 3. Аугментация

#### `xray_alpha_dense()`

Функция для X-ray Alpha аугментации с референсным фоном и low-freq + texture плотностью.

**Особенности:**
- Убрана бинарная вырезка - используется `eps = 1/255` для сохранения тонких частей
- Добавлен референсный фон (низкочастотное поле, не ноль)
- Плотность объекта = low-freq + texture (экспоненциальная модуляция)

**Параметры:**
- `obj_floor_range=(0.35, 0.55)` - базовый уровень плотности объекта
- `obj_amp_range=(0.25, 0.45)` - амплитуда модуляции от low-freq
- `obj_max_range=(0.85, 0.97)` - максимальная плотность (ceiling) для избежания "белой заливки"
- `detail_k_range=(0.6, 1.2)` - коэффициент экспоненциальной модуляции деталей
- `bg_base_range=(0.08, 0.16)` - базовый уровень фона (~20..40 в uint8)
- `bg_noise_amp=0.02` - амплитуда шума для фона
- `bg_noise_sigma=64.0` - очень мягкий шум для фона
- `low_freq_sigma=10.0` - sigma для low-freq размытия
- `texture_thick_sigma_range=(0.6, 1.2)` - мягкое утолщение texture (не убивает мелочь)
- `unsharp_amount_range=(0.3, 0.6)` - unsharp для micro-контраста внутри объекта

**Алгоритм:**
1. Multi-scale DoG для texture (мелкие и крупные детали)
2. Мягкое утолщение texture после объединения масштабов
3. Экспоненциальная модуляция для сильного контраста
4. Нормировка texture по "ядру" объекта (без краёв)
5. Unsharp на density для micro-контраста
6. Ceiling для плотности объекта чтобы избежать "белой заливки"

#### `JointAugment`

Класс для совместной аугментации изображений и масок.

**Параметры:**
- `size=512` - размер кропа
- `p_flip=0.5` - вероятность горизонтального отражения
- `p_rotate=0.3` - вероятность поворота
- `max_rotate=10` - максимальный угол поворота (градусы)
- `p_color=0.7` - вероятность цветовой аугментации

**Аугментации:**
- Random crop
- Horizontal flip
- Rotation (с заполнением)
- Color jitter (brightness, contrast, saturation) - только для изображений

### 4. Dataset

#### `CsvAlphaDataset`

Датасет для загрузки изображений и масок из CSV файла.

**CSV формат:**
- Колонка `image_path` - путь к RGB изображению
- Колонка `alpha_path` - путь к alpha маске (grayscale)

**Параметры:**
- `csv_path` - путь к CSV файлу
- `size=(1024, 1024)` - размер изображений
- `normalize=True` - нормализация ImageNet (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
- `augment` - объект `JointAugment` или `None`
- `limit` - ограничение количества данных
- Параметры x-ray аугментации (см. `xray_alpha_dense`)

**Возвращает:**
- `{"img": tensor, "mask": tensor}` - словарь с тензорами изображения и маски

**Особенности:**
- X-ray аугментация применяется ДО трансформаций в тензоры (работа с PIL/numpy)
- Сохраняются тонкие части и полутона (не бинарная маска)

#### `TestImageDataset`

Датасет для тестовых изображений (без масок).

**Параметры:**
- `root` - директория с тестовыми изображениями
- `size=(1024, 1024)` - размер изображений

**Возвращает:**
- `{"path": filename, "img": tensor}` - словарь с именем файла и тензором изображения

### 5. Loss функции

#### `u2net_mse_loss()`

Улучшенный loss для U2NETP с фокусом на внутреннюю структуру объекта.

**Компоненты loss:**
1. **Base MSE** (вес 1.0) - основной MSE loss
2. **Object Weighted MSE** (вес 0.8) - детализация внутри объекта через weighted MSE с весом от градиента GT
   - Усиливает MSE там, где у GT есть изменения (градиенты)
   - Коэффициент усиления `k = 2.0` для зон с градиентами
3. **Edge Loss** (вес 0.1) - совпадение градиентов (границы)
4. **Background Loss** (вес 0.5) - жёсткий штраф за "туман" на фоне
   - Штраф за значения > 0.02 на фоне
   - MSE по фону

**Особенности:**
- Обрабатывает множественные выходы модели (U2NETP имеет несколько выходов)
- Усредняет loss по всем выходам

#### `u2net_mse_metric()`

Метрика MSE по главному выходу (d0) для валидации.

#### Вспомогательные функции

- `u2net_outputs_to_list()` - преобразует выходы модели в список
- `sobel()` - Sobel оператор для вычисления градиентов (краёв)
- `sobel_mag()` - магнитуда градиента Sobel
- `erode()` - эрозия маски через min pooling
- `thin_map()` - создаёт карту тонких зон (тонкие детали + края)
- `charbonnier()` - Charbonnier loss (smooth L1)

### 6. Модель

**Архитектура:** U2NETP (U-2-Net)

**Инициализация:**
- Загружается предобученная модель из `U-2-Net/saved_models/u2netp/u2netp.pth`
- Модель инициализируется с `in_ch=3, out_ch=1`
- Переводится на GPU/CPU в зависимости от доступности

### 7. Обучение

#### `train_epoch()`

Функция для обучения модели на одной эпохе.

**Процесс:**
1. Переводит модель в режим обучения (`model.train()`)
2. Итерируется по батчам с прогресс-баром
3. Вычисляет loss через `u2net_mse_loss()`
4. Выполняет обратное распространение и обновление весов
5. Поддерживает AMP (Automatic Mixed Precision) - отключено по умолчанию
6. Проверяет на non-finite loss

**Возвращает:**
- `(train_loss, train_mse)` - средний loss и MSE по эпохе

#### `eval_epoch()`

Функция для валидации модели на одной эпохе.

**Процесс:**
1. Переводит модель в режим оценки (`model.eval()`)
2. Итерируется по батчам без градиентов (`torch.no_grad()`)
3. Вычисляет loss и метрику MSE
4. Проверяет на non-finite loss

**Возвращает:**
- `(val_loss, val_mse)` - средний loss и MSE по эпохе

#### Оптимизатор и Scheduler

**Оптимизатор:** AdamW
- `lr=5e-5`
- `weight_decay=1e-4`
- `betas=(0.9, 0.999)`
- `eps=1e-8`

**Scheduler:** ReduceLROnPlateau
- `mode="min"` - уменьшает LR при уменьшении метрики
- `factor=0.1` - коэффициент уменьшения LR
- `patience=1` - количество эпох без улучшения перед уменьшением LR
- `min_lr=1e-7` - минимальный learning rate

#### Цикл обучения

**Логика:**
- Если `RUN_TRAINING = True`: запускается обучение на `NUM_EPOCHS` эпох
- Если `RUN_TRAINING = False`: загружается последняя сохранённая модель

**Сохранение моделей:**
- Сохраняется лучшая модель по `val_mse`
- Используется функция `get_unique_path()` для уникальных имён файлов
- Формат: `u2netp_best.pth`, `u2netp_best_2.pth`, и т.д.

**Вывод:**
- После каждой эпохи выводится: `Epoch {epoch}: train_mse={...} val_mse={...} lr={...}`
- При сохранении лучшей модели: `-> Сохранена лучшая модель: {filename} (val_mse={...})`

### 8. Инференс и Submission

#### Процесс инференса

1. **Подготовка:**
   - Модель переводится в режим оценки (`model.eval()`)
   - Создаётся `TestImageDataset` и `DataLoader`

2. **Flip-TTA (Test Time Augmentation):**
   - Предсказание на оригинальном изображении
   - Предсказание на горизонтально отражённом изображении
   - Усреднение предсказаний: `probs = (probs_orig + probs_flip) / 2.0`

3. **Постпроцессинг:**
   - Компонентный анализ с мягкими критериями:
     - Порог поддержки: `0.05` (низкий для захвата всех объектов)
     - Находятся компоненты связности через `scipy.ndimage.label()`
     - Оцениваются компоненты по `max_conf` и `area_frac`
     - Оставляются top-3 компонентов с критериями:
       - `max_conf > 0.20` ИЛИ `area_frac > 0.01` (1% от изображения)
   - Дилатация маски поддержки (kernel_size=7) для мягких краёв
   - Применение дилатированной маски к предикту (сохраняет детали)

4. **Сохранение:**
   - Конвертация в uint8: `mask = (pred_np * 255).astype(np.uint8)`
   - Сохранение как PNG в BytesIO
   - Base64 кодирование
   - Добавление в список строк: `{"filename": name, "image_utf": base64_string}`

5. **Создание CSV:**
   - Создаётся DataFrame из списка строк
   - Сохранение в `MODEL_DIR / "submission.csv"` (с уникальным именем)
   - Формат: колонки `filename` и `image_utf`

## Использование

### Запуск обучения

```bash
python 01_clean.py
```

### Изменение параметров

Отредактируйте переменные в секции CONFIG:
- `RUN_TRAINING = True` - для обучения
- `RUN_TRAINING = False` - только инференс
- `LIMIT_DATA = None` - для использования всех данных
- `NUM_EPOCHS = 12` - количество эпох
- `LR = 5e-5` - learning rate

### Визуализация

Код визуализации закомментирован в конце файла. Для использования раскомментируйте секцию с визуализацией submission.csv.

## Особенности реализации

1. **X-ray аугментация:** Создаёт реалистичные полутона внутри объектов, имитируя рентгеновские снимки
2. **Weighted Loss:** Фокус на внутреннюю структуру объекта через градиенты GT
3. **Flip-TTA:** Улучшает качество предсказаний через усреднение
4. **Компонентный постпроцесс:** Фильтрует шум и оставляет только значимые объекты
5. **Уникальные имена файлов:** Автоматическое создание уникальных имён для моделей и submission файлов

## Зависимости

- PyTorch
- torchvision
- PIL (Pillow)
- numpy
- pandas
- scipy
- opencv-python (cv2)
- tqdm
- matplotlib (опционально)

## Структура выходных файлов

```
outputs/experiments/exp_001/
├── u2netp_best.pth          # Лучшая модель (или u2netp_best_2.pth, ...)
└── submission.csv            # Submission файл (или submission_2.csv, ...)
```

## Примечания

- Скрипт автоматически создаёт директорию `MODEL_DIR` если её нет
- При обучении модель сохраняется только если улучшается `val_mse`
- При инференсе загружается последняя сохранённая модель (с наибольшим номером)
- Визуализация отключена по умолчанию (закомментирована)

