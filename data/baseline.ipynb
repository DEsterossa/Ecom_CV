{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959610b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import base64\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# отключаем ограничение PIL для больших изображений\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "# фиксируем сиды для воспроизводимости\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# пути и базовые настройки\n",
    "DATA_ROOT = ...\n",
    "TEST_ROOT = ...\n",
    "MODEL_DIR = ...\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = (1024, 1024)\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "LR = 1e-4\n",
    "RUN_TRAINING = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33aa111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root,\n",
    "        size = (1024, 1024),\n",
    "        mask_subfolder = 'gt',\n",
    "        image_subfolder = 'im',\n",
    "        image_format = '.jpg',\n",
    "        mask_format = '.png',\n",
    "        num_mask_channels = 1\n",
    "    ):\n",
    "        self.size = size\n",
    "\n",
    "        self.data_root = Path(data_root)\n",
    "        if not self.data_root.exists():\n",
    "            raise ValueError(\"Instance images root doesn't exists.\")\n",
    "\n",
    "        self.mask_subfolder = mask_subfolder\n",
    "        self.image_subfolder = image_subfolder\n",
    "        self.image_format = image_format\n",
    "        self.mask_format = mask_format\n",
    "        self.num_mask_channels = num_mask_channels\n",
    "\n",
    "        if image_format is None and mask_format is None:\n",
    "            self.data = os.listdir(str(self.data_root / self.mask_subfolder))\n",
    "        else:\n",
    "            self.data = [i.rsplit('.')[0] for i in os.listdir(str(self.data_root / self.mask_subfolder)) if i.endswith(self.mask_format)]\n",
    "        \n",
    "        self._length = len(self.data)\n",
    "        \n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]), # 0 - 1 to -1 - 1\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.num_mask_channels == 3:\n",
    "            self.mask_transforms = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.5], [0.5])\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.mask_transforms = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                    transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        obj = self.data[index]\n",
    "        \n",
    "        mask = Image.open(self.data_root / self.mask_subfolder / f'{obj}{self.mask_format if self.mask_format is not None else \"\"}')\n",
    "        if self.num_mask_channels == 3:\n",
    "            mask = mask.convert('RGB')\n",
    "        img = Image.open(self.data_root / self.image_subfolder / f'{obj}{self.image_format if self.image_format is not None else \"\"}').convert('RGB')\n",
    "\n",
    "        mask = self.mask_transforms(mask)\n",
    "        img = self.image_transforms(img)\n",
    "        example = {\n",
    "            \"mask\": mask,\n",
    "            \"img\": img\n",
    "        }\n",
    "\n",
    "        return example\n",
    "\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, root: Path, size=(1024, 1024)):\n",
    "        self.root = Path(root)\n",
    "        self.images = sorted([p for p in self.root.iterdir() if p.is_file()])\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.images[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return {\n",
    "            \"path\": path.name,\n",
    "            \"img\": self.transform(img),\n",
    "        }\n",
    "\n",
    "\n",
    "def get_dataloaders(train_root: Path, size=(1024, 1024), batch_size=4):\n",
    "    train_dataset = SegmentationDataset(\n",
    "        data_root=train_root,\n",
    "        size=size,\n",
    "        mask_subfolder=\"gt\",\n",
    "        image_subfolder=\"im\",\n",
    "        image_format=\".jpg\",\n",
    "        mask_format=\".png\",\n",
    "        num_mask_channels=1,\n",
    "    )\n",
    "    val_size = max(1, int(0.1 * len(train_dataset)))\n",
    "    train_size = len(train_dataset) - val_size\n",
    "    train_ds, val_ds = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9005190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, base_ch=32):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_channels, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch * 2)\n",
    "        self.enc3 = ConvBlock(base_ch * 2, base_ch * 4)\n",
    "        self.enc4 = ConvBlock(base_ch * 4, base_ch * 8)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = ConvBlock(base_ch * 8, base_ch * 16)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(base_ch * 16, base_ch * 8, kernel_size=2, stride=2)\n",
    "        self.dec4 = ConvBlock(base_ch * 16, base_ch * 8)\n",
    "        self.up3 = nn.ConvTranspose2d(base_ch * 8, base_ch * 4, kernel_size=2, stride=2)\n",
    "        self.dec3 = ConvBlock(base_ch * 8, base_ch * 4)\n",
    "        self.up2 = nn.ConvTranspose2d(base_ch * 4, base_ch * 2, kernel_size=2, stride=2)\n",
    "        self.dec2 = ConvBlock(base_ch * 4, base_ch * 2)\n",
    "        self.up1 = nn.ConvTranspose2d(base_ch * 2, base_ch, kernel_size=2, stride=2)\n",
    "        self.dec1 = ConvBlock(base_ch * 2, base_ch)\n",
    "\n",
    "        self.head = nn.Conv2d(base_ch, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "\n",
    "        d4 = self.up4(b)\n",
    "        d4 = torch.cat([d4, e4], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "\n",
    "        d3 = self.up3(d4)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        return self.head(d1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_metric(logits, target):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    return F.mse_loss(probs, target)\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_mse = 0.0\n",
    "    for batch in tqdm(loader, desc=\"train\", leave=False):\n",
    "        imgs = batch[\"img\"].to(device)\n",
    "        masks = batch[\"mask\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        total_mse += mse_metric(logits, masks).item() * imgs.size(0)\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss / n, total_mse / n\n",
    "\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mse = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"val\", leave=False):\n",
    "            imgs = batch[\"img\"].to(device)\n",
    "            masks = batch[\"mask\"].to(device)\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, masks)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            total_mse += mse_metric(logits, masks).item() * imgs.size(0)\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss / n, total_mse / n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea65648",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_dataloaders(DATA_ROOT, size=IMG_SIZE, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = UNet(in_channels=3, out_channels=1, base_ch=32).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "best_val_mse = float(\"inf\")\n",
    "best_path = MODEL_DIR / \"unet_dis_best.pth\"\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        train_loss, train_mse = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss, val_mse = eval_epoch(model, val_loader, criterion)\n",
    "        print(f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_mse={val_mse:.4f}\")\n",
    "        if val_mse < best_val_mse:\n",
    "            best_val_mse = val_mse\n",
    "            torch.save({\"model_state\": model.state_dict()}, best_path)\n",
    "else:\n",
    "    if best_path.exists():\n",
    "        model.load_state_dict(torch.load(best_path)[\"model_state\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_dataset = TestImageDataset(TEST_ROOT, size=(1024, 1024))\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "rows = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"test\", leave=False):\n",
    "        imgs = batch[\"img\"].to(device)\n",
    "        names = batch[\"path\"]\n",
    "        logits = model(imgs)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        mask = (probs[0, 0].cpu().numpy() * 255.0).clip(0, 255).astype(np.uint8)\n",
    "        print(mask.shape)\n",
    "        pil_mask = Image.fromarray(mask, mode=\"L\")\n",
    "        buf = io.BytesIO()\n",
    "        pil_mask.save(buf, format=\"PNG\")\n",
    "        image_utf = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
    "        rows.append({\"filename\": names[0].split(\".\")[0], \"image_utf\": image_utf})\n",
    "\n",
    "submission = pd.DataFrame(rows)\n",
    "submission_path = MODEL_DIR / \"submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"Saved submission to {submission_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
